{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DUMAY_KALLEL_2RecommendationALS.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QcsEKsXnRI7Q",
        "plk3D6jMXDsw",
        "rFSNTD3qd_l1"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSP47cgQP2FT",
        "colab_type": "text"
      },
      "source": [
        "# Prediction of music listening (Part II): Recommendation with Alternating Least Squares\n",
        "\n",
        "In this notebook, we aim to make predictions based on the Weighted Alternating Least Squares model. The outputs of the WALS can be used as a latent representation for both users and items. \n",
        "\n",
        "We feed these latent representation to various ML models in notebook III.\n",
        "\n",
        "\n",
        "You should note that this notebook and all the following were implemented on Google Colab, which is quicker than Jupyter in our case. If you want to open a Google Colab session, [here is the link](https://colab.research.google.com/notebooks/welcome.ipynb#recent=true).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcsEKsXnRI7Q",
        "colab_type": "text"
      },
      "source": [
        "### Package installation\n",
        "\n",
        "\n",
        "*   **Implicit** is a package used to provide fast Python implementations of several different popular recommendation algorithms for implicit feedback datasets. It enables us to implement Alternating Least Squares.\n",
        "*   **Kaggle** is used to import data from Kaggle (our dataset is from a Kaggle challenge)\n",
        "*   **Pandas v.0.21** is a package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive.\n",
        "\n",
        "To import a Kaggle dataset, you have to generate a token. In order to do so,you need to have a Kaggle account for accessing Kaggle API. You can find the token on your Kaggle account page. Once you download the token to your local machine, you can copy the token in the notebook. [Here is the link to access the Kaggle page of the challenge.](https://www.kaggle.com/c/kkbox-music-recommendation-challenge/overview)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yR6AaGTbFVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --user implicit\n",
        "!pip install --user -q kaggle\n",
        "!pip install --user pandas==0.21 ## This version is necessary for implicit package to work\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXA9vTvzT7d3",
        "colab_type": "text"
      },
      "source": [
        "After that, we want to load the dataset, create a directory and copy the data from Kaggle. In the provided dataset, we will use train.csv (and unzip it).\n",
        "\n",
        "The test set is mainly for competition submissions and thus doesn't contain targets. \n",
        "\n",
        "Therefore we can't use the test set for local model evaluation.\n",
        "\n",
        "We split the train set in order to do our local evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0riRqBgDc-IN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## You need to download a kaggle token from your personal computer in order to download dataset from kaggle\n",
        "## HELP ==>\n",
        "#https://adityashrm21.github.io/Setting-Up-Kaggle/\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIczQcdcdLkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!kaggle competitions download -c kkbox-music-recommendation-challenge\n",
        "!7za x train.csv.7z # unzip the file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plk3D6jMXDsw",
        "colab_type": "text"
      },
      "source": [
        "## Rearanging data\n",
        "In order to be able to evaluate our results, we must compare datasets which are \"similar\". In the test dataset, we only want to have songs that appear in the training dataset. If not, we couldn't predict the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYAnIoLJGgXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc0r0r3Gd-ao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('train.csv') # read the data\n",
        "df_train, df_test = train_test_split(df, test_size = 0.2) #split into training and test dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21m36TXJXpGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Clear out tuples with either song or user that didn't figure in the training dataset.\n",
        "\n",
        "df_test = df_test[df_test['song_id'].isin(df_train['song_id'])]\n",
        "df_test = df_test[df_test['msno'].isin(df_train['msno'])]\n",
        "\n",
        "print('df_test shape before cleaning: ', df_test.shape[0])\n",
        "print('df_test shape after cleaning: ', df_test.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg-mnGURShnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How many null values do we have in each column ?\n",
        "\n",
        "percent_missing = df_test.isnull().sum() * 100 / len(df_test)\n",
        "missing_value_df = pd.DataFrame({'column_name': df_test.columns,\n",
        "                                 'percent_missing': percent_missing})\n",
        "print ('Number of missing values per column of the test set')\n",
        "missing_value_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYOHeIVPZR4h",
        "colab_type": "text"
      },
      "source": [
        "Another problem we are facing is the usability of the data provided. Initialy, UserID are strings, as SongID. We rename the colums and transform it into numerical attributes, that are easier to manipulate.\n",
        "\n",
        "\n",
        "In a second time, we need to have the same indexes in both the test dataset and the training dataset, to be able to compare the two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc_lJ4cZeA4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train['SongID'] = df_train.groupby(['song_id']).ngroup()\n",
        "df_train['UserID'] = df_train.groupby(['msno']).ngroup()\n",
        "\n",
        "print('We have ', df_train['UserID'].nunique(), 'unique users.')\n",
        "print('We have ', df_train['SongID'].nunique(), 'unique songs.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA0sEfnfs5Ft",
        "colab_type": "text"
      },
      "source": [
        "**Note:** One problem we stumbled upon when using the IMPLICIT package is that User_ids and song ids must be continuous in values. This is why we have to get set IDs for training first then merge the Test set with training on those ids.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY6J_LfoEqmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The indexes must be the same in training and testing datasets\n",
        "\n",
        "df_train_ids = df_train.groupby('song_id').first().reset_index()\n",
        "df_train_ids = df_train_ids[['msno', 'song_id', 'SongID', 'UserID']]\n",
        "\n",
        "df_test = df_test[['target', 'song_id', 'msno']]\n",
        "\n",
        "df_test_ids = pd.merge(df_test, df_train_ids, left_on = 'msno', right_on = 'msno').drop(['msno'], axis=1)\n",
        "df_test_ids = pd.merge(df_test, df_train_ids, left_on = 'song_id', right_on = 'song_id').drop(['song_id'], axis=1)\n",
        "\n",
        "df_train = df_train[['UserID', 'SongID', 'target']]\n",
        "\n",
        "df_test = df_test_ids[['UserID', 'SongID', 'target']]\n",
        "\n",
        "print('df test shape = ',df_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFSNTD3qd_l1",
        "colab_type": "text"
      },
      "source": [
        "## Alternating Least Squares & latent factors\n",
        "In order to use Alternating Least Squares more efficiently, we will transform the training dataset into a CSR matrix. \n",
        "\n",
        "\n",
        "CSR stands for Compressed Sparse Row matrix. Sparse matrices can be used in efficient arithmetic operations: they support addition, subtraction, multiplication, division, and matrix power.\n",
        "\n",
        "In this section, we will use latent-factor models. They try to explain observed interactions between large numbers of users and songs through a relatively small number of observations.\n",
        "\n",
        "First, we formulate the learning problem as a matrix completion problem. Then, we will use a type of matrix factorization model to \"fill in\" the blanks. We are given implicit ratings that users have given certain items (if they listened a song again or not) and our goal is to predict their ratings for the rest of the items. Formally, if there are $n$ users and $m$ items, we are given an $n \\times m$ matrix $R$ in which the generic entry $(u, i)$ represents the rating for item $i$ by user $u$. Matrix $R$ has many missing entries indicating unobserved ratings, and our task is to estimate these unobserved ratings.\n",
        "\n",
        "A popular approach to the matrix completion problem is matrix factorization, where we want to \"summarize\" users and items with their latent factors. For that, we approximate the initial matrix $R$ by the product of two smaller matrices $X$ and $Y$.\n",
        "\n",
        "The challenge is to calculate $X$ and $Y$. We do this iteratively: knowing $Y$, we can calculate the best value of $X$, and vice versa. It means from the initial values of $X$ and $Y$ in the beginning, we calculate the best $X$ according to $Y$, and then calculate the best $Y$ according to the new $X$. This process is repeated until the distance from $XY$ to $R$ is small.\n",
        "\n",
        "The values composing $X$ and $Y$ are called latent factors.\n",
        "\n",
        "To use Alternating Least Squares, we first need to intialize the model, then train it on the sparse matrix we just created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZUjNYShjxQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "import implicit\n",
        "import multiprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSPPBBjFnGn4",
        "colab_type": "text"
      },
      "source": [
        "We are using the library **multiprocessing**.  This package offers both local and remote concurrency, solving Global Interpreter problems by using subprocesses instead of threads. Due to this, the multiprocessing module allows the programmer to fully leverage multiple processors on a given machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT2dxTJuaNVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "user_item_data = csr_matrix((df_train['target'], (df_train['UserID'], df_train['SongID'])), \\\n",
        "                           shape = (df_train['UserID'].nunique(), df_train['SongID'].nunique()))\n",
        "\n",
        "user_item_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZYYaQcDbGdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize \n",
        "model = implicit.als.AlternatingLeastSquares(factors = 64)\n",
        "\n",
        "# Train\n",
        "model.fit(user_item_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KtQgjbJtzdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.recommend_all(user_item_data,10)  This function would have saved us a lot of trouble but it doesnt work"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJAMeZQKhnQd",
        "colab_type": "text"
      },
      "source": [
        "Now we want to do user re-listenings predictions. \n",
        "\n",
        "To do so, we check if the song figures in the N top recommended songs for the user, and if so, we suppose it's target is 1.\n",
        "\n",
        "Unfortunately this approach was inconclusive as our model will always predict false even for N= 1000000\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE5SYk76Z0eF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da973c62-e8d3-4d32-ef8a-ec8ab56ed829"
      },
      "source": [
        "## This function return a boolean to know if the current song is recommended for the current user\n",
        "\n",
        "## It takes in input a song, a user, and the number of recommendations to genrate\n",
        "\n",
        "\n",
        "def predict(user_id, song_id,filter_already_liked_items=False, N = 10000):\n",
        "    r = model.recommend(user_id, user_item_data, N = N)\n",
        "    recommendations = [i[0] for i in r] #  get recommended songID without the score associated\n",
        "    return (song_id in recommendations)\n",
        "\n",
        "\n",
        "predict (5951,260925) ## This is a sample from the test dataset (it should be True)\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RV2MfbIgs4l",
        "colab_type": "code",
        "outputId": "1f3fb656-e8db-44a8-d80f-d5621ac69e26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "\n",
        "args=[[UserID,SongID] for UserID, SongID, target in df_test.sample(frac=0.01).head(1000).values]\n",
        "\n",
        "#multiprocessing\n",
        "try:\n",
        "    cpus = multiprocessing.cpu_count()\n",
        "except NotImplementedError:\n",
        "    cpus = 2   # default\n",
        "print('cpus = ', cpus)\n",
        "\n",
        "pool = multiprocessing.Pool(processes = cpus) # start multiple worker processes\n",
        "\n",
        "predictions=pool.starmap(predict, args)\n",
        "\n",
        "print(predictions)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpus =  4\n",
            "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfP065Oj1wCQ",
        "colab_type": "text"
      },
      "source": [
        "In this part we get the latent item / user representations and merge them with the dataset. The resulting dataframe is used in next notebooks to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "masdVV8HeW7r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "d324173a-a39c-412f-9f12-9c6814ce5953"
      },
      "source": [
        "item_latent = pd.DataFrame(model.item_factors)\n",
        "user_latent = pd.DataFrame(model.user_factors)\n",
        "user_latent.columns = ['user_latent' + str(x) for i, x in enumerate(user_latent.columns, 1)]\n",
        "item_latent.columns = ['item_latent' + str(x) for i, x in enumerate(item_latent.columns, 1)]\n",
        "item_latent['ID'] = item_latent.index\n",
        "user_latent['ID'] = user_latent.index\n",
        "\n",
        "user_latent.head()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_latent0</th>\n",
              "      <th>user_latent1</th>\n",
              "      <th>user_latent2</th>\n",
              "      <th>user_latent3</th>\n",
              "      <th>user_latent4</th>\n",
              "      <th>user_latent5</th>\n",
              "      <th>user_latent6</th>\n",
              "      <th>user_latent7</th>\n",
              "      <th>user_latent8</th>\n",
              "      <th>user_latent9</th>\n",
              "      <th>...</th>\n",
              "      <th>user_latent55</th>\n",
              "      <th>user_latent56</th>\n",
              "      <th>user_latent57</th>\n",
              "      <th>user_latent58</th>\n",
              "      <th>user_latent59</th>\n",
              "      <th>user_latent60</th>\n",
              "      <th>user_latent61</th>\n",
              "      <th>user_latent62</th>\n",
              "      <th>user_latent63</th>\n",
              "      <th>ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-2.644544e-12</td>\n",
              "      <td>-9.249792e-13</td>\n",
              "      <td>-3.224912e-13</td>\n",
              "      <td>-9.439513e-13</td>\n",
              "      <td>7.236304e-13</td>\n",
              "      <td>-8.306632e-13</td>\n",
              "      <td>9.881727e-13</td>\n",
              "      <td>-8.107907e-13</td>\n",
              "      <td>-1.547658e-12</td>\n",
              "      <td>-2.536470e-12</td>\n",
              "      <td>...</td>\n",
              "      <td>3.000360e-12</td>\n",
              "      <td>8.705412e-13</td>\n",
              "      <td>-1.765695e-12</td>\n",
              "      <td>2.107446e-13</td>\n",
              "      <td>1.371772e-12</td>\n",
              "      <td>1.115754e-12</td>\n",
              "      <td>2.426915e-13</td>\n",
              "      <td>1.950971e-12</td>\n",
              "      <td>-4.237625e-13</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-6.598627e-13</td>\n",
              "      <td>-1.196054e-12</td>\n",
              "      <td>-1.430170e-12</td>\n",
              "      <td>-4.840771e-13</td>\n",
              "      <td>6.661880e-13</td>\n",
              "      <td>4.118457e-13</td>\n",
              "      <td>4.349186e-13</td>\n",
              "      <td>-1.709455e-12</td>\n",
              "      <td>-1.480153e-12</td>\n",
              "      <td>-1.868602e-12</td>\n",
              "      <td>...</td>\n",
              "      <td>1.899065e-12</td>\n",
              "      <td>3.959238e-13</td>\n",
              "      <td>3.158488e-13</td>\n",
              "      <td>1.452306e-12</td>\n",
              "      <td>1.577157e-12</td>\n",
              "      <td>-2.245885e-14</td>\n",
              "      <td>2.923200e-13</td>\n",
              "      <td>2.977683e-12</td>\n",
              "      <td>8.951582e-13</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.087132e-11</td>\n",
              "      <td>3.323642e-11</td>\n",
              "      <td>1.069165e-11</td>\n",
              "      <td>3.920881e-11</td>\n",
              "      <td>2.010811e-11</td>\n",
              "      <td>1.108030e-11</td>\n",
              "      <td>4.999447e-11</td>\n",
              "      <td>4.102699e-12</td>\n",
              "      <td>6.302878e-11</td>\n",
              "      <td>1.389113e-10</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.744835e-11</td>\n",
              "      <td>4.669810e-11</td>\n",
              "      <td>-4.482312e-11</td>\n",
              "      <td>-1.043379e-11</td>\n",
              "      <td>-1.895363e-10</td>\n",
              "      <td>-7.028418e-11</td>\n",
              "      <td>2.652573e-12</td>\n",
              "      <td>-3.908648e-11</td>\n",
              "      <td>2.607690e-11</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.644166e-12</td>\n",
              "      <td>3.255939e-11</td>\n",
              "      <td>3.780337e-12</td>\n",
              "      <td>1.902603e-12</td>\n",
              "      <td>-1.780760e-11</td>\n",
              "      <td>-1.469769e-11</td>\n",
              "      <td>6.033555e-12</td>\n",
              "      <td>4.034762e-11</td>\n",
              "      <td>6.017527e-11</td>\n",
              "      <td>2.160263e-11</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.878170e-11</td>\n",
              "      <td>4.839207e-12</td>\n",
              "      <td>1.866528e-11</td>\n",
              "      <td>-1.427836e-11</td>\n",
              "      <td>-3.688095e-11</td>\n",
              "      <td>-2.994064e-11</td>\n",
              "      <td>-1.850649e-11</td>\n",
              "      <td>-6.332652e-11</td>\n",
              "      <td>-7.296884e-12</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-4.988765e-12</td>\n",
              "      <td>-5.361404e-12</td>\n",
              "      <td>-5.259260e-12</td>\n",
              "      <td>-3.595482e-12</td>\n",
              "      <td>6.883490e-12</td>\n",
              "      <td>-3.511613e-13</td>\n",
              "      <td>6.941035e-12</td>\n",
              "      <td>-6.516514e-12</td>\n",
              "      <td>-9.911166e-12</td>\n",
              "      <td>-6.946741e-12</td>\n",
              "      <td>...</td>\n",
              "      <td>1.433093e-11</td>\n",
              "      <td>5.181223e-12</td>\n",
              "      <td>-3.812514e-13</td>\n",
              "      <td>5.413257e-12</td>\n",
              "      <td>5.837914e-12</td>\n",
              "      <td>1.335183e-12</td>\n",
              "      <td>2.625753e-12</td>\n",
              "      <td>9.709965e-12</td>\n",
              "      <td>4.123598e-12</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 65 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_latent0  user_latent1  user_latent2  user_latent3  user_latent4  \\\n",
              "0 -2.644544e-12 -9.249792e-13 -3.224912e-13 -9.439513e-13  7.236304e-13   \n",
              "1 -6.598627e-13 -1.196054e-12 -1.430170e-12 -4.840771e-13  6.661880e-13   \n",
              "2  8.087132e-11  3.323642e-11  1.069165e-11  3.920881e-11  2.010811e-11   \n",
              "3 -1.644166e-12  3.255939e-11  3.780337e-12  1.902603e-12 -1.780760e-11   \n",
              "4 -4.988765e-12 -5.361404e-12 -5.259260e-12 -3.595482e-12  6.883490e-12   \n",
              "\n",
              "   user_latent5  user_latent6  user_latent7  user_latent8  user_latent9 ...  \\\n",
              "0 -8.306632e-13  9.881727e-13 -8.107907e-13 -1.547658e-12 -2.536470e-12 ...   \n",
              "1  4.118457e-13  4.349186e-13 -1.709455e-12 -1.480153e-12 -1.868602e-12 ...   \n",
              "2  1.108030e-11  4.999447e-11  4.102699e-12  6.302878e-11  1.389113e-10 ...   \n",
              "3 -1.469769e-11  6.033555e-12  4.034762e-11  6.017527e-11  2.160263e-11 ...   \n",
              "4 -3.511613e-13  6.941035e-12 -6.516514e-12 -9.911166e-12 -6.946741e-12 ...   \n",
              "\n",
              "   user_latent55  user_latent56  user_latent57  user_latent58  user_latent59  \\\n",
              "0   3.000360e-12   8.705412e-13  -1.765695e-12   2.107446e-13   1.371772e-12   \n",
              "1   1.899065e-12   3.959238e-13   3.158488e-13   1.452306e-12   1.577157e-12   \n",
              "2  -2.744835e-11   4.669810e-11  -4.482312e-11  -1.043379e-11  -1.895363e-10   \n",
              "3  -2.878170e-11   4.839207e-12   1.866528e-11  -1.427836e-11  -3.688095e-11   \n",
              "4   1.433093e-11   5.181223e-12  -3.812514e-13   5.413257e-12   5.837914e-12   \n",
              "\n",
              "   user_latent60  user_latent61  user_latent62  user_latent63  ID  \n",
              "0   1.115754e-12   2.426915e-13   1.950971e-12  -4.237625e-13   0  \n",
              "1  -2.245885e-14   2.923200e-13   2.977683e-12   8.951582e-13   1  \n",
              "2  -7.028418e-11   2.652573e-12  -3.908648e-11   2.607690e-11   2  \n",
              "3  -2.994064e-11  -1.850649e-11  -6.332652e-11  -7.296884e-12   3  \n",
              "4   1.335183e-12   2.625753e-12   9.709965e-12   4.123598e-12   4  \n",
              "\n",
              "[5 rows x 65 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8uRnd1uSDVu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "196145b6-b069-45e4-fb20-98377dc71bd4"
      },
      "source": [
        "df_mini = df_test.sample(frac = 1)\n",
        "print('df mini shape = ', df_mini.shape)\n",
        "print('df mini size = ', df_mini.count())\n",
        "\n",
        "df_latent = pd.merge(df_mini, item_latent, left_on = 'UserID', right_on = 'ID').drop(['ID'], axis = 1)\n",
        "df_latent = pd.merge(df_latent, user_latent, left_on = 'SongID', right_on = 'ID').drop(['ID'], axis = 1)\n",
        "print('df latent shape after second merge: ', df_latent.shape)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df mini shape =  (1436893, 3)\n",
            "df mini size =  UserID    1436893\n",
            "SongID    1436893\n",
            "target    1436893\n",
            "dtype: int64\n",
            "df latent shape after second merge:  (1436893, 131)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}